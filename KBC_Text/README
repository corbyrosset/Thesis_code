Code for Corby's thesis on jointly embedding knowledge graphs and textual instances of their triples

TODO: adagrad
TODO: Nent = 16296 when it should be 14295 (1345 rels). Truncate the original FB15k data, and make this ubiquitous throughout. Fix data matrices to have Nent rows or Nrel rows only! In Utils, Operations, everywhere

TODO: average of word embeddings was not taking into account the entities at all!!!
TODO: create a "high quality" textual triples dataset that filters out
	sentences that are 
	1) labeled with too many different triples (bc theyre not indicative 
		of a unique relationship)
	2) labeled with more than one relationship?
	3) Trigram and Precision filtering of Toutanova: I counted for each 
	pattern and each trigram in a pattern (trigram being a consecutive trigram 
	in the dependency path including words and dependency links), the total 
	number of occurrence. The precision for pattern or pattern trigram P is 
	defined as follows:  for every entity pair e1,e2  which appears with P we 
	record whether e1,e2 has any KB relation (in the training KB). Then the 
	proportion of such e1,e2 that have a KB relation is defined as the 
	precision. I had cutoffs on patterns and pattern trigrams, where if a 
	pattern passed the cutoff or if one of its trigrams passed the cutoff, the 
	pattern was included in the pruned set.

TODO: integrate the SIF word average embedding class into the 
	FB15k_text_experiment file. (perhaps forget the RMPC part, since that can't
	be part of backprop)
TODO: use weightfile from paragrams code to actually do weighted average of 
	words in sentence

-----------------------------------------------------------------
DONE:

1. early stopping on validation MRR after 30 epochs
2. specify ranking based on whether margin aims to minize score of a triple or 
	maximize it
3. regularize relations, not entities (split between left and right 
	relalation, if they exist)
4. different optimization algorithms 
5. integrating textual mentions, using them as relation or regularization, or both


1. Filtering Clueweb Annotations down to FB15k
-----------------------------------------------------------------

Please run explore_comparisons.ipynb under '/Users/corbinrosset/Dropbox/Arora/QA-code/src/process_clueweb/' to generate this output

Comparing Relationships between FB15k and Clueweb Triples...
	Total number of relationships (triples have many instances, and a given textual instance may express up to 2 relationships): 195792926
	FB15k relationship cardinality: 1389 ### 1345? some relationships have a ‘.’
	Clueweb relationship cardinality: 6549
	common relationships between FB15k and Clueweb: 1328
	Number of textual instances expressing a common relationship: 177567142
Comparing Entities between FB15k and Clueweb...
	Total number of entities mentioned (each triple has two entities): 267068018
	Total number of textual instances (half of the above): 133534009
	Cardinality of Clueweb entities: 895829
	Cardinality of FB15k entities: 14951
	common entities between FB15k and Clueweb: 14003
	Number of textual instances expressing at least one common entity: 210231218

Clueweb to FB15k script:
Please run '/Users/corbinrosset/Dropbox/Arora/QA-code/src/process_clueweb/Clueweb_to_FB15k.py’ to generate these outputs.

It produced 47,291,696 textual instances that represent valid FB15k triples AND that don’t have null text after pre-processing

Number of unique strings: 6,194,853
total successes: 47,291,696
LHS: (16296, 47291696) <class 'scipy.sparse.csc.csc_matrix'>
RHS: (16296, 47291696)
REL: (16296, 47291696)
SENT: (47291696,)

That is, only about 35% (47M) of the 133M textual triples remain after projecting down to the entity/relation space of FB15k and text processing. And those 47M are comprised of only 6M unique textual instances. The text processing is done in compute_stats_and_structures.py to transform raw clue web files into “*_final” files, and the script to project onto FB15k space (and remove nulls) is clueweb_to_FB15k.py

If you were to use the raw, unprocessed clue web text “*_processed” files, this script would generate 77,439,346 instances with 7,738,607 unique strings. 

Number of unique strings: 7,738,607
total successes: 77,439,346
LHS: (16296, 77439346) <class 'scipy.sparse.csc.csc_matrix'>
RHS: (16296, 77439346)
REL: (16296, 77439346)
SENT: (77439346,)
done writing triple components to their respective files

There are also many textual mentions that are just lists of entities, like countries or states. We filtered out 609,043 such sentences 
	"mongolia malawi turkey mountain pass california"
	"california oregon texas arkansas"
	"poland czech republic estonia latvia lithuania"

Furthermore, we filter the clueweb triples based on how many distinct relationships and triples a sentence was labeled to express:

for a triple to be included its sentence must:
	1) be involved in at most 4 distinct relations
	2) be involved in at most 8 distinct tripels 
	3) have number of words at least 2 and at most 25

These conditions leave only 4,416,201 out of 6,194,853 sentences, 
leaving 17,355,291 out of 47,291,696 triples

The data in this filtered dataset is labeled as 
	clueweb_FB15k_all-lhs_filtered.pkl
	clueweb_FB15k_all-rhs_filtered.pkl
	clueweb_FB15k_all-rel_filtered.pkl
	clueweb_FB15k_all-sent_filtered.pkl





The user can rank left and right entities, and optionally relations, during 
training and during test. There are parameters for how many triples to
rank after how many epochs in the main training loop.

-----------------------------------------------------------------
-----------------------------------------------------------------
-----------------------------------------------------------------
-----------------------------------------------------------------



===
Although not used, the architecture of SME has been designed by **Xavier Glorot** (https://github.com/glorotxa), with some contributions from **Antoine Bordes** (https://www.hds.utc.fr/~bordesan).

**Update (Nov 13):** the code for Translating Embeddings (see https://everest.hds.utc.fr/doku.php?id=en:transe) has been included along with a new version for Freebase (FB15k).

1. Overview
-----------------------------------------------------------------

This package proposes scripts using Theano to perform training and evaluation on several datasets of the models: 
- **Structured Embeddings** (SE) defined in (Bordes et al., AAAI 2011);
- **Semantic Matching Energy** (SME_lin & SME_bil) defined in (Bordes et al., MLJ 2013);
- **Translating Embeddings** (TransE) defined in (Bordes et al., NIPS 2013).
- **TATEC** defined in (Garcia-Duran et al., ECML14, arxiv15).

For each of these, the training models are all implemented in model.py. I think this is rather clunky...


Please refer to the following pages for more details and references:  
- https://everest.hds.utc.fr/doku.php?id=en:smemlj12
- https://everest.hds.utc.fr/doku.php?id=en:transe
- https://everest.hds.utc.fr/doku.php?id=en:2and3ways

Content of the package:
- model.py : contains the classes and functions to create the different models and Theano functions (training, evaluation...).
- {dataset}_exp.py : contains an experiment function to train all the different models on a given dataset.
- The data/ folder contains the data files for the learning scripts.
- in the {dataset}/ folders:
	* {dataset}_parse.py : parses and creates data files for the training script of a given dataset.
	* {dataset}_evaluation.py : contains evaluation functions for a given dataset.
	* {dataset}\_{model_name}.py : runs the best hyperparameters experiment for a given dataset and a given model.
	* {dataset}\_{model_name}.out : output we obtained on our machines for a given dataset and a given model using the script above.
	* {dataset}_test.py : perform quick runs for small models of all types to test the scripts.

The datasets currently available are:
 * **Multi-relational benchmarks** (Kinhsips, UMLS & Nations -- Tensor folder) to be downloaded from https://everest.hds.utc.fr/doku.php?id=en:smemlj12
 * **WordNet** (WN folder) to be downloaded from https://everest.hds.utc.fr/doku.php?id=en:smemlj12
 * **Freebase** (FB folder) used in (Bordes et al., AAAI 2011) to be downloaded from https://everest.hds.utc.fr/doku.php?id=en:smemlj12
 * **Freebase15k** (FB15k folder)  used in (Bordes et al., NIPS 2013) to be downloaded from https://everest.hds.utc.fr/doku.php?id=en:transe
 * **Synthethic family database** (Family folder) user is (Garcia-Duran et al. arxiv15a) to be downloaded from https://everest.hds.utc.fr/doku.php?id=en:2and3ways



2. 3rd Party Libraries
-----------------------------------------------------------------

You need to install Theano to use those scripts. It also requires: Python >= 2.4, Numpy >=1.5.0, Scipy>=0.8.
The experiment scripts are compatible with Jobman but this library is not mandatory.


3. Installation
-----------------------------------------------------------------

Put the script folder in your PYTHONPATH.


4. Data Files Creation
-----------------------------------------------------------------

Put the absolute path of the downloaded dataset (from: https://everest.hds.utc.fr/doku.php?id=en:smemlj12 or  https://everest.hds.utc.fr/doku.php?id=en:transe) at the beginning of the {dataset}_parse.py script and run it (the SME folder has to be your current directory). Note: Running Tensor_parse.py generates data for both Kinhsips, UMLS & Nations.

5. Training and Evaluating a Model
-----------------------------------------------------------------

Simply run the corresponding {dataset}_{model_name}.py file (the SME/{dataset}/ folder has to be your current directory) to launch a training. When it's over, running {dataset}_evaluation.py with the path to the best_valid_model.pkl of the learned model runs the evaluation on the test set

6. Citing
-----------------------------------------------------------------

If you use this code, you could provide the link to the github page: https://github.com/glorotxa/SME . Also, depending on the model used, you should cite either the paper on **Structured Embeddings** (Bordes et al., AAAI 2011), on **Semantic Matching Energy** (Bordes et al., MLJ 2013) or on **Translating Embeddings** (Bordes et al., NIPS 2013).

7. References
-----------------------------------------------------------------

This code is based on https://github.com/glorotxa/SME. See below:

- (Garcia-Duran et al., arxiv 15) *Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases* Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier and Yves Grandvalet. http://arxiv.org/abs/1506.00999
- (Bordes et al., NIPS 2013) *Translating Embeddings for Modeling Multi-relational Data* (2013). Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston and Oksana Yakhnenko. In Proceedings of Neural Information Processing Systems (NIPS 26), Lake Taho, NV, USA. Dec. 2013.
- (Bordes et al., MLJ 2013) *A Semantic Matching Energy Function for Learning with Multi-relational Data* (2013). Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. in Machine Learning. Springer, DOI: 10.1007/s10994-013-5363-6, May 2013
- (Bordes et al., AAAI 2011) *Learning Structured Embeddings of Knowledge Bases* (2011). Antoine Bordes, Jason Weston, Ronan Collobert and Yoshua Bengio. in Proceedings of the 25th Conference on Artificial Intelligence (AAAI), AAAI Press.


