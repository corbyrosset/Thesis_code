{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FB15k-test-rel.pkl\n",
    "# FB15k-train-lhs.pkl\n",
    "# FB15k-train-rhs.pkl\n",
    "# FB15k_idx2entity.pkl\n",
    "# FB15k_entity2idx.pkl\n",
    "\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pprint\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rhs = pickle.load(open('FB15k-train-rhs.pkl', 'r')).toarray()\n",
    "lhs = pickle.load(open('FB15k-train-lhs.pkl', 'r')).toarray()\n",
    "rel = pickle.load(open('FB15k-train-rel.pkl', 'r')).toarray()\n",
    "entity2idx = pickle.load(open('../data/FB15k_entity2idx.pkl', 'r'))\n",
    "idx2entity = pickle.load(open('../data/FB15k_idx2entity.pkl', 'r'))\n",
    "\n",
    "data_path = '/Users/corbinrosset/Dropbox/Arora/QA-data/VanDurme_FB_annotations/annotated_clueweb/ClueWeb09_English_1/processed/'\n",
    "execute_path = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/process_clueweb/'\n",
    "manifest_file = 'manifest.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(entity2idx.keys())\n",
    "print len(idx2entity.keys())\n",
    "print entity2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import cPickle\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Put the freebase15k data absolute path here\n",
    "datapath = '/Users/corbinrosset/Desktop/QA_datasets/FB15k/'\n",
    "assert datapath is not None\n",
    "\n",
    "if 'data' not in os.listdir('../'):\n",
    "    os.mkdir('../data')\n",
    "\n",
    "\n",
    "def parseline(line):\n",
    "    lhs, rel, rhs = line.split('\\t')\n",
    "    lhs = lhs.split(' ')\n",
    "    rhs = rhs.split(' ')\n",
    "    rel = rel.split(' ')\n",
    "    return lhs, rel, rhs\n",
    "\n",
    "#################################################\n",
    "### Creation of the entities/indices dictionnaries\n",
    "\n",
    "np.random.seed(753)\n",
    "\n",
    "entities1 = set([])\n",
    "entleftlist = []\n",
    "entrightlist = []\n",
    "rellist = []\n",
    "relset1 = set([])\n",
    "\n",
    "for datatyp in ['train']:\n",
    "    f = open(datapath + 'freebase_mtr100_mte100-%s.txt' % datatyp, 'r')\n",
    "    dat = f.readlines()\n",
    "    f.close()\n",
    "    for i in dat:\n",
    "        lhs, rel, rhs = parseline(i[:-1])\n",
    "        entleftlist += [lhs[0]]\n",
    "        entrightlist += [rhs[0]]\n",
    "        rellist += [rel[0]]\n",
    "        \n",
    "        rel = rel[0].replace('/', '.').strip('.')\n",
    "        if '..' in rel:\n",
    "            temp = rel.split('..')\n",
    "            relset1.add(temp[0])\n",
    "            relset1.add(temp[1])\n",
    "        else:\n",
    "            relset1.add(rel)\n",
    "        \n",
    "        entities1.add(lhs[0].replace('/', '.').strip('.'))\n",
    "        entities1.add(rhs[0].replace('/', '.').strip('.'))\n",
    "\n",
    "entleftset = np.sort(list(set(entleftlist) - set(entrightlist)))\n",
    "entsharedset = np.sort(list(set(entleftlist) & set(entrightlist)))\n",
    "entrightset = np.sort(list(set(entrightlist) - set(entleftlist)))\n",
    "relset = np.sort(list(set(rellist)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# todo: filter clueweb triples to include only those available in FB15k...\n",
    "###############################################################################\n",
    "###         Global Settings and Paths  for Clueweb Data                     ###\n",
    "###############################################################################\n",
    "\n",
    "### relationship similarities:\n",
    "print 'Comparing Relationships between FB15k and Clueweb Triples...'\n",
    "rltn_freq = pickle.load(open(data_path + 'rltn_freq.pkl', 'r'))\n",
    "relset2 =  set(rltn_freq.keys())\n",
    "total_clueweb_triples = sum(rltn_freq.values())\n",
    "print '\\tTotal number of relationships (triples have many instances, and a given textual instance may express up to 2 relationships): ' + str(total_clueweb_triples)\n",
    "\n",
    "overlap_rel = relset1 & relset2\n",
    "print '\\tFB15k relationship cardinality: ' + str(len(relset1))\n",
    "print '\\tClueweb relationship cardinality: ' + str(len(relset2))\n",
    "sym_dif = relset1 ^ relset2\n",
    "diff1 = relset2 - relset1\n",
    "diff2 = relset1 - relset2\n",
    "print '\\tcommon relationships between FB15k and Clueweb: ' + str(len(overlap_rel))\n",
    "print '\\tNumber of textual instances expressing a common relationship: ' + str(sum([rltn_freq[i] for i in overlap_rel])) \n",
    "# pprint.pprint(diff2)\n",
    "\n",
    "\n",
    "### entities\n",
    "print 'Comparing Entities between FB15k and Clueweb...'\n",
    "enty_freq = pickle.load(open(data_path + 'enty_id_freq.pkl', 'r'))\n",
    "entities2 = set(enty_freq.keys())\n",
    "total_entity_mentions = sum(enty_freq.values())\n",
    "print '\\tTotal number of entities mentioned (each triple has two entities): ' + str(total_entity_mentions)\n",
    "print '\\tTotal number of textual instances (half of the above): ' + str(total_entity_mentions/2)\n",
    "print '\\tCardinality of Clueweb entities: ' + str(len(entities2))\n",
    "print '\\tCardinality of FB15k entities: ' + str(len(entities1))\n",
    "\n",
    "overlap_enty = entities1 & entities2\n",
    "sym_dif = entities1 ^ entities2\n",
    "diff1 = entities2 - entities1\n",
    "diff2 = entities1 - entities2\n",
    "print '\\tcommon entities between FB15k and Clueweb: ' + str(len(overlap_enty))\n",
    "print '\\tNumber of textual instances expressing at least one common entity: ' + str(sum([enty_freq[i] for i in overlap_enty])) \n",
    "# pprint.pprint(diff2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded manifest of which data files need to be processed...\n",
      "processing en0000.final_0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a326916d38e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'no \\\".processed\\\" file exists, run parse_text_anchors_vandurme.py on a corpus'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_data_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a326916d38e9>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(formatted_data_chunk, chunk)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtriple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleftID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelationship\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrightID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mleftID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mrightID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process(formatted_data_chunk, chunk):\n",
    "    for i, line in enumerate(formatted_data_chunk):\n",
    "        ### format of each line:\n",
    "        ### [relationships, reversd, (leftEntity, leftEntityID), result, (rightEntity, rightEntityID)]\n",
    "        line = ast.literal_eval(line) ### convert text to data structure\n",
    "        reversd = int(line[1])\n",
    "        relationships = line[0]\n",
    "        left = line[2]\n",
    "        text = line[3] \n",
    "        right = line[4]\n",
    "        \n",
    "        leftID = '/' + left[1].replace('.', '/')\n",
    "        rightID = '/' + right[1].replace('.', '/')\n",
    "        relationship = '/' + './'.join([r.replace('.', '/') for r in relationships])\n",
    "        \n",
    "        triple = (leftID, relationship, rightID)\n",
    "        \n",
    "        if (leftID not in entities_set) or (rightID not in entities_set):\n",
    "            print triple\n",
    "\n",
    "    print 'all entities are valid'\n",
    "        \n",
    "#         print ''\n",
    "#         if leftID not in entity2idx:\n",
    "#             print str(i) + ' left not valid ' + str(triple)\n",
    "#         if rightID not in entity2idx:\n",
    "#             print str(i) + ' right not valid ' + str(triple)\n",
    "#         if relationship not in entity2idx:\n",
    "#             print str(i) + ' rel not valid ' + str(triple)\n",
    "            \n",
    "        \n",
    "    \n",
    "    \n",
    "enty_freq = pickle.load(open(data_path + 'enty_id_freq.pkl', 'r'))\n",
    "entities_set = set(enty_freq.keys())\n",
    "\n",
    "### open manifest of formatted data\n",
    "USE_FINAL = True\n",
    "try:\n",
    "    m = open(data_path + manifest_file, 'r')\n",
    "    manifest_list = m.readlines()\n",
    "    print 'loaded manifest of which data files need to be processed...'\n",
    "except:\n",
    "    print 'no manifest file, aborting'\n",
    "    exit()\n",
    "\n",
    "### collect statistics of the corpus chunk by chunk\n",
    "for chunk in manifest_list[:1]:\n",
    "    chunk = chunk.strip()\n",
    "    if USE_FINAL: \n",
    "        chunk = chunk.replace('processed', 'final') ### use only final files\n",
    "    try:\n",
    "        formatted_data_chunk = open(data_path + chunk, 'r')\n",
    "        print 'processing ' + str(chunk)\n",
    "    except:\n",
    "        print 'no \\\".processed\\\" file exists, run parse_text_anchors_vandurme.py on a corpus'\n",
    "        continue\n",
    "    process(formatted_data_chunk, chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
