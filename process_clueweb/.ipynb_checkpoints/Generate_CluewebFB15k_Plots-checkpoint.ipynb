{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### notebook to analyze textual similarity of clueweb instances that share the same entity-relationship-entity triple\n",
    "### uses the 5000 most common triples which are stored in \n",
    "### /Users/corbinrosset/Dropbox/Arora/QA-data/VanDurme_FB_annotations/annotated_clueweb/ClueWeb09_English_1/processed/grouped_textual_triples.pkl\n",
    "\n",
    "\n",
    "'''\n",
    "Only look at \"clueweb_FB15k_all-<lhs, rhs, rel, sent>.pkl\" files\n",
    "or perhaps the raw versions thereof. \n",
    "\n",
    "Investigate overlap of Clueweb textual mentions and FB15k triples\n",
    "- How many unique textual mentions are associated with each triple\n",
    " \t- with the most common triples (have the most examples)\n",
    " \t- then investigate how paraphrastic these unique textual mentions are for \n",
    " \t  a given triple (triple is the label - distant supervision)\n",
    "- which and how many unique text strings are associated with many triples?\n",
    "\t- if there are many, it confounds the assumption that text is \n",
    "\t  representative of the triples they are mentions of\n",
    "\n",
    "'''\n",
    "from __future__ import division\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import operator\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "###############################################################################\n",
    "###                         \tGlobals                                     ###\n",
    "###############################################################################\n",
    "\n",
    "data_path = '/Users/corbinrosset/Dropbox/Arora/QA-data/FB15k_Clueweb/processed_text/'\n",
    "# FB15k_path = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/TransE_Text/data/'\n",
    "execute_path = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/process_clueweb/'\n",
    "manifest_file = 'manifest.txt'\n",
    "input_prefix = 'grouped_FB15k_clueweb_triples'\n",
    "# grouped_FB15k_clueweb_triples-counts.pkl\n",
    "# grouped_FB15k_clueweb_triples-text_sets.pkl\n",
    "\n",
    "datatyp = 'all' # which .pkl files to load\n",
    "NUM_FB1K_TRIPLES = 47291696\n",
    "NUM_UNIQUE_SENTENCES = 6194853\n",
    "\n",
    "entity2idx = None \n",
    "idx2entity = None \n",
    "num_entities_union_rels = 0 # sum of |Entities| + |Relatiionships| in FB15k\n",
    "USE_FINAL = False #True\n",
    "min_words_per_text = 2 ### min number of words needed to count a textual triple\n",
    "num_triples = 5000 ### put top num_triples into the triples_to_extract below\n",
    "\n",
    "\n",
    "unique_sent_map = {}\n",
    "idx_2_sent_map = {}\n",
    "\n",
    "text_per_triple_cntr = {} # count total number of textual instances f.e. triple\n",
    "unique_text_per_triple = {} # set of unique text mentions f.e. triple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### load data\n",
    "start = time.clock() \n",
    "\n",
    "text_per_triple_cntr = pickle.load(open(data_path + input_prefix + '-counts.pkl', 'r'))\n",
    "unique_text_per_triple = pickle.load(open(data_path + input_prefix + '-text_sets.pkl', 'r'))\n",
    "triple_per_text = pickle.load(open(data_path + input_prefix + '-triple_per_text_count.pkl', 'r'))\n",
    "entity2idx = pickle.load(open(data_path + 'FB15k_entity2idx.pkl', 'r'))\n",
    "idx2entity = pickle.load(open(data_path + 'FB15k_idx2entity.pkl', 'r'))\n",
    "unique_sent_map = pickle.load(open(data_path + 'clueweb_FB15k_' + 'all-sent2idx.pkl', 'r'))\n",
    "idx_2_sent_map = pickle.load(open(data_path + 'clueweb_FB15k_' + 'all-idx2sent.pkl', 'r'))\n",
    "num_entities_union_rels = np.max(entity2idx.values()) + 1\n",
    "srtd_triples = sorted(text_per_triple_cntr, key=text_per_triple_cntr.__getitem__, reverse=True)\n",
    "\n",
    "elapsed = time.clock()\n",
    "elapsed = elapsed - start\n",
    "print \"loaded data in: \", elapsed\n",
    "assert len(unique_text_per_triple.keys()) == len(text_per_triple_cntr.keys())\n",
    "print 'Number of unique triples: ' + str(len(unique_text_per_triple.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rels_I_want = ['food', 'medicine', 'spaceflight', 'architecture', 'education', \\\n",
    "               'business', 'biology', 'olympics', 'aviation', \\\n",
    "               'music', 'chemistry', 'law', 'religion', \\\n",
    "               'spaceflight']\n",
    "\n",
    "counter = 0\n",
    "print 'starting'\n",
    "for i in srtd_triples:\n",
    "    if rels_I_want[14] not in idx2entity[i[1]]:\n",
    "        continue\n",
    "    if counter > 1000:\n",
    "        break\n",
    "    counter += 1\n",
    "    avg_triple_per_text = np.average([triple_per_text[j] for j in unique_text_per_triple[i]])\n",
    "    print '\\t' + str(idx2entity[i[0]]) + ' ' + str(idx2entity[i[1]]) \\\n",
    "        + ' ' + str(idx2entity[i[2]]) + ' count: ' + str(text_per_triple_cntr[i]) \\\n",
    "        + ' num unique texts: ' + str(len(unique_text_per_triple[i])) \\\n",
    "        + ' average number of other triples associated with the sentences here: ' + str(avg_triple_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### show hist of number of different TRIPLES assigned to each text instance\n",
    "triples_per_text_srtd = [np.ceil(np.log2(i)) for i in triple_per_text.values()]\n",
    "cntr = Counter(triples_per_text_srtd)\n",
    "plt.hist(triples_per_text_srtd, bins=20)\n",
    "plt.xlabel('log base 2 of the number of distinct triples assigned to a textual instance (rounded up)')\n",
    "plt.ylabel('Number of Textual Instances')\n",
    "plt.title('Histogram of the number of triples assigned to each textual instance')\n",
    "plt.show()\n",
    "print cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### show hist of number of different RELATIONS assigned to each text instance\n",
    "triples_per_text_srtd = [np.ceil(np.log2(i)) for i in triple_per_text.values()]\n",
    "cntr = Counter(triples_per_text_srtd)\n",
    "plt.hist(triples_per_text_srtd, bins=20)\n",
    "plt.xlabel('log base 2 of the number of distinct triples assigned to a textual instance (rounded up)')\n",
    "plt.ylabel('Number of Textual Instances')\n",
    "plt.title('Histogram of the number of triples assigned to each textual instance')\n",
    "plt.show()\n",
    "print cntr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
