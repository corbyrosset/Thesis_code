{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### notebook to analyze textual similarity of clueweb instances that share the same entity-relationship-entity triple\n",
    "### uses the 5000 most common triples which are stored in \n",
    "### /Users/corbinrosset/Dropbox/Arora/QA-data/VanDurme_FB_annotations/annotated_clueweb/ClueWeb09_English_1/processed/grouped_textual_triples.pkl\n",
    "\n",
    "\n",
    "'''\n",
    "Only look at \"clueweb_FB15k_all-<lhs, rhs, rel, sent>.pkl\" files\n",
    "or perhaps the raw versions thereof. \n",
    "\n",
    "Investigate overlap of Clueweb textual mentions and FB15k triples\n",
    "- How many unique textual mentions are associated with each triple\n",
    " \t- with the most common triples (have the most examples)\n",
    " \t- then investigate how paraphrastic these unique textual mentions are for \n",
    " \t  a given triple (triple is the label - distant supervision)\n",
    "- which and how many unique text strings are associated with many triples?\n",
    "\t- if there are many, it confounds the assumption that text is \n",
    "\t  representative of the triples they are mentions of\n",
    "\n",
    "'''\n",
    "from __future__ import division\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import operator\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "###############################################################################\n",
    "###                         \tGlobals                                     ###\n",
    "###############################################################################\n",
    "\n",
    "data_path = '/Users/corbinrosset/Dropbox/Arora/QA-data/FB15k_Clueweb/processed_text/'\n",
    "# FB15k_path = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/TransE_Text/data/'\n",
    "execute_path = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/process_clueweb/'\n",
    "manifest_file = 'manifest.txt'\n",
    "input_prefix = 'grouped_FB15k_clueweb_triples'\n",
    "# grouped_FB15k_clueweb_triples-counts.pkl\n",
    "# grouped_FB15k_clueweb_triples-text_sets.pkl\n",
    "\n",
    "datatyp = 'all' # which .pkl files to load\n",
    "NUM_FB1K_TRIPLES = 47291696\n",
    "NUM_UNIQUE_SENTENCES = 6194853\n",
    "\n",
    "entity2idx = None \n",
    "idx2entity = None \n",
    "num_entities_union_rels = 0 # sum of |Entities| + |Relatiionships| in FB15k\n",
    "USE_FINAL = False #True\n",
    "min_words_per_text = 2 ### min number of words needed to count a textual triple\n",
    "num_triples = 5000 ### put top num_triples into the triples_to_extract below\n",
    "\n",
    "\n",
    "unique_sent_map = {}\n",
    "idx_2_sent_map = {}\n",
    "\n",
    "text_per_triple_cntr = {} # count total number of textual instances f.e. triple\n",
    "unique_text_per_triple = {} # set of unique text mentions f.e. triple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/corbinrosset/Dropbox/Arora/QA-data/FB15k_Clueweb/processed_text/grouped_FB15k_clueweb_triples-triple_per_text_counts.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-15be8d715ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_per_triple_cntr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-counts.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munique_text_per_triple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-text_sets.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtriple_per_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-triple_per_text_counts.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mentity2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FB15k_entity2idx.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0midx2entity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'FB15k_idx2entity.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/corbinrosset/Dropbox/Arora/QA-data/FB15k_Clueweb/processed_text/grouped_FB15k_clueweb_triples-triple_per_text_counts.pkl'"
     ]
    }
   ],
   "source": [
    "### load data\n",
    "start = time.clock() \n",
    "\n",
    "text_per_triple_cntr = pickle.load(open(data_path + input_prefix + '-counts.pkl', 'r'))\n",
    "unique_text_per_triple = pickle.load(open(data_path + input_prefix + '-text_sets.pkl', 'r'))\n",
    "triple_per_text = pickle.load(open(data_path + input_prefix + '-triple_per_text_count.pkl', 'r'))\n",
    "entity2idx = pickle.load(open(data_path + 'FB15k_entity2idx.pkl', 'r'))\n",
    "idx2entity = pickle.load(open(data_path + 'FB15k_idx2entity.pkl', 'r'))\n",
    "unique_sent_map = pickle.load(open(data_path + 'clueweb_FB15k_' + 'all-sent2idx.pkl', 'r'))\n",
    "idx_2_sent_map = pickle.load(open(data_path + 'clueweb_FB15k_' + 'all-idx2sent.pkl', 'r'))\n",
    "num_entities_union_rels = np.max(entity2idx.values()) + 1\n",
    "srtd_triples = sorted(text_per_triple_cntr, key=text_per_triple_cntr.__getitem__, reverse=True)\n",
    "\n",
    "elapsed = time.clock()\n",
    "elapsed = elapsed - start\n",
    "print \"loaded data in: \", elapsed\n",
    "assert len(unique_text_per_triple.keys()) == len(text_per_triple_cntr.keys())\n",
    "print 'Number of unique triples: ' + str(len(unique_text_per_triple.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    }
   ],
   "source": [
    "rels_I_want = ['food', 'medicine', 'spaceflight', 'architecture', 'education', \\\n",
    "               'business', 'biology', 'olympics', 'aviation', \\\n",
    "               'music', 'chemistry', 'law', 'religion', \\\n",
    "               'spaceflight']\n",
    "\n",
    "counter = 0\n",
    "print 'starting'\n",
    "for i in srtd_triples:\n",
    "    if rels_I_want[14] not in idx2entity[i[1]]:\n",
    "        continue\n",
    "    if counter > 1000:\n",
    "        break\n",
    "    counter += 1\n",
    "    print '\\t' + str(idx2entity[i[0]]) + ' ' + str(idx2entity[i[1]]) \\\n",
    "        + ' ' + str(idx2entity[i[2]]) + ' count: ' + str(text_per_triple_cntr[i]) \\\n",
    "        + ' num unique texts: ' + str(len(unique_text_per_triple[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional or an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5bb21b59383f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     counter = 1000 ### <= 1000 2nd positions chosen for each 1st position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:15359)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional or an integer"
     ]
    }
   ],
   "source": [
    "### show hist of number of different triples assigned to each text instance\n",
    "plt.plot(sorted(triple_per_text.values(), reverse=True))\n",
    "plt.ylabel('Unique triples assigned to each textual instance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
