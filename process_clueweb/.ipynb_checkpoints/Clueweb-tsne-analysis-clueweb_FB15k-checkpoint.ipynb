{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import pickle, sys\n",
    "sys.path.append('/Users/corbinrosset/Dropbox/Arora/QA-code/src/WordEmdgs/SIF/src/')\n",
    "import data_io # eval\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "from scipy.stats import iqr\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import gzip, cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tsne import bh_sne\n",
    "import mpld3\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "### notebook to analyze textual similarity of clueweb instances that share the same entity-relationship-entity triple\n",
    "### uses the 5000 most common triples which are stored in \n",
    "### /Users/corbinrosset/Dropbox/Arora/QA-data/VanDurme_FB_annotations/annotated_clueweb/ClueWeb09_English_1/processed/grouped_textual_triples.pkl\n",
    "\n",
    "\n",
    "'''\n",
    "Only look at \"clueweb_FB15k_all-<lhs, rhs, rel, sent>.pkl\" files\n",
    "or perhaps the raw versions thereof. \n",
    "\n",
    "Investigate overlap of Clueweb textual mentions and FB15k triples\n",
    "- How many unique textual mentions are associated with each triple\n",
    " \t- with the most common triples (have the most examples)\n",
    " \t- then investigate how paraphrastic these unique textual mentions are for \n",
    " \t  a given triple (triple is the label - distant supervision)\n",
    "- which and how many unique text strings are associated with many triples?\n",
    "\t- if there are many, it confounds the assumption that text is \n",
    "\t  representative of the triples they are mentions of\n",
    "\n",
    "'''\n",
    "\n",
    "###############################################################################\n",
    "###                         \tGlobals                                     ###\n",
    "###############################################################################\n",
    "\n",
    "data_path = '/Users/corbinrosset/Dropbox/Arora/QA-data/FB15k_Clueweb/processed_text/'\n",
    "execute_path = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/process_clueweb/'\n",
    "manifest_file = 'manifest.txt'\n",
    "input_prefix = 'grouped_FB15k_clueweb_triples'\n",
    "paraphrase_file = 'common_triples_FB15k_clueweb_1.txt'\n",
    "\n",
    "datatyp = 'all' # which .pkl files to load\n",
    "NUM_FB1K_TRIPLES = 47291696\n",
    "NUM_UNIQUE_SENTENCES = 6194853\n",
    "\n",
    "entity2idx = None \n",
    "idx2entity = None \n",
    "num_entities_union_rels = 0 # sum of |Entities| + |Relatiionships| in FB15k\n",
    "USE_FINAL = False #True\n",
    "min_words_per_text = 2 ### min number of words needed to count a textual triple\n",
    "num_triples = 5000 ### put top num_triples into the triples_to_extract below\n",
    "\n",
    "unique_sent_map = {}\n",
    "idx_2_sent_map = {}\n",
    "text_per_triple_cntr = {} # count total number of textual instances f.e. triple\n",
    "unique_text_per_triple = {} # set of unique text mentions f.e. triple\n",
    "\n",
    "\n",
    "### the following is the word embeddings maps to use, download them pretrained\n",
    "wordfiles = [#'../data/paragram_sl999_small.txt', # need to download it from John Wieting's github (https://github.com/jwieting/iclr2016)\n",
    "    # '/Users/corbinrosset/Dropbox/GloVe/glove.840B.300d.txt'\n",
    "    '/Users/corbinrosset/Dropbox/Paragrams/paragrams-XXL-SL999.txt'\n",
    "\n",
    "    ]\n",
    "weightfile = '/Users/corbinrosset/Dropbox/Arora/QA-code/src/WordEmdgs/SIF/auxiliary_data/enwiki_vocab_min200.txt'\n",
    "weightparas = [1e-2] ### used 1e-2 #[-1,1e-1,1e-2,1e-3,1e-4] # (0.01 works best)\n",
    "# weightparas = [-1,1e-1,1e-2,1e-3,1e-4] # (0.01 works best)\n",
    "\n",
    "rmpcs = [0] #[0, 1, 2, 3, 4] # remove the first k principle components, (4 is best)\n",
    "scoring_function = None\n",
    "save_result = False \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "###############################################################################\n",
    "###                         Helper Classes                                  ###\n",
    "###############################################################################\n",
    "\n",
    "class params(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.LW = 1e-5\n",
    "        self.LC = 1e-5\n",
    "        self.eta = 0.05\n",
    "\n",
    "    def __str__(self):\n",
    "        t = \"LW\", self.LW, \", LC\", self.LC, \", eta\", self.eta\n",
    "        t = map(str, t)\n",
    "        return ' '.join(t)\n",
    "\n",
    "\n",
    "params = params()\n",
    "parr4para = {}\n",
    "sarr4para = {}\n",
    "wordfile = wordfiles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data in:  369.099122\n",
      "Number of unique triples: 325337\n"
     ]
    }
   ],
   "source": [
    "### load clueweb FB15k data\n",
    "start = time.clock() \n",
    "\n",
    "text_per_triple_cntr = pickle.load(open(data_path + input_prefix + '-counts.pkl', 'r'))\n",
    "unique_text_per_triple = pickle.load(open(data_path + input_prefix + '-text_sets.pkl', 'r'))\n",
    "triple_per_text = pickle.load(open(data_path + input_prefix + '-triple_per_text_count.pkl', 'r'))\n",
    "entity2idx = pickle.load(open(data_path + 'FB15k_entity2idx.pkl', 'r'))\n",
    "idx2entity = pickle.load(open(data_path + 'FB15k_idx2entity.pkl', 'r'))\n",
    "unique_sent_map = pickle.load(open(data_path + 'clueweb_FB15k_' + 'all-sent2idx.pkl', 'r'))\n",
    "idx_2_sent_map = pickle.load(open(data_path + 'clueweb_FB15k_' + 'all-idx2sent.pkl', 'r'))\n",
    "# num_entities_union_rels = np.max(entity2idx.values()) + 1\n",
    "srtd_triples = sorted(text_per_triple_cntr, key=text_per_triple_cntr.__getitem__, reverse=True)\n",
    "\n",
    "elapsed = time.clock()\n",
    "elapsed = elapsed - start\n",
    "print \"loaded data in: \", elapsed\n",
    "assert len(unique_text_per_triple.keys()) == len(text_per_triple_cntr.keys())\n",
    "print 'Number of unique triples: ' + str(len(unique_text_per_triple.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings map...will take many minutes\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3c1f4ca38c27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m'word vectors loaded from %s in %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwordfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "### load word embeddings map\n",
    "print 'loading word embeddings map...will take many minutes'\n",
    "start = time.clock() \n",
    "(words, We) = data_io.getWordmap(wordfile)\n",
    "elapsed = time.clock()\n",
    "elapsed = elapsed - start\n",
    "print 'word vectors loaded from ' + str(wordfile) + ' in ' + str(elapsed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "###            Helper Methods for the Original Textual Similarity Task      ###\n",
    "###############################################################################\n",
    "\n",
    "def evaluate_similarity(We, words, f, weight4ind, scoring_function, params):\n",
    "    golds = []\n",
    "    seq1 = []\n",
    "    seq2 = []\n",
    "    with open(f,'r') as fl:\n",
    "        for i in fl:\n",
    "            i = i.split(\"\\t\")\n",
    "            p1 = i[0]; p2 = i[1]; score = float(i[2])\n",
    "            X1, X2 = data_io.getSeqs(p1,p2,words)\n",
    "            seq1.append(X1)\n",
    "            seq2.append(X2)\n",
    "            golds.append(score)\n",
    "    x1,m1 = data_io.prepare_data(seq1)\n",
    "    x2,m2 = data_io.prepare_data(seq2)\n",
    "    m1 = data_io.seq2weight(x1, m1, weight4ind)\n",
    "    m2 = data_io.seq2weight(x2, m2, weight4ind)\n",
    "    scores = scoring_function(We,x1,x2,m1,m2, params)\n",
    "    preds = np.squeeze(scores)\n",
    "    return pearsonr(preds,golds)[0], spearmanr(preds,golds)[0]\n",
    "\n",
    "def get_embeddings(We, words, data, weight4ind, params):\n",
    "\n",
    "    ### asssuming data is the similarity dataset:\n",
    "    seq, labels, sentences = [], [], []\n",
    "    with open(data,'r') as fl:\n",
    "        for idx, i in enumerate(fl):\n",
    "            i = i.split(\"\\t\")\n",
    "            p1 = i[0]; p2 = i[1]; score = float(i[2])\n",
    "            X1, X2 = data_io.getSeqs(p1,p2,words)\n",
    "            seq.append(X1)\n",
    "            seq.append(X2)\n",
    "            labels.append(score)\n",
    "            labels.append(score)\n",
    "            sentences.append(p1)\n",
    "            sentences.append(p2)\n",
    "\n",
    "    x1, w1 = data_io.prepare_data(seq)\n",
    "    emb, embnorm = get_norm_embedding(We, x1, w1, params)\n",
    "    emb = np.asarray(emb, dtype=np.float64)\n",
    "    # perhaps divide each row by its L2 norm, emb /= embnorm\n",
    "    print np.shape(emb)\n",
    "    return emb, labels, sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###                      SIF Averaging Methods                              ###\n",
    "###############################################################################\n",
    "\n",
    "def get_weighted_average(We, x, w):\n",
    "    \"\"\"\n",
    "    Compute the weighted average vectors\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x: x[i, :] are the indices of the words in sentence i\n",
    "    :param w: w[i, :] are the weights for the words in sentence i\n",
    "    :return: emb[i, :] are the weighted average vector for sentence i\n",
    "    \"\"\"\n",
    "    n_samples = x.shape[0]\n",
    "    emb = np.zeros((n_samples, We.shape[1]))\n",
    "    for i in xrange(n_samples):\n",
    "        emb[i,:] = w[i,:].dot(We[x[i,:],:]) / np.count_nonzero(w[i,:])\n",
    "    return emb\n",
    "\n",
    "def compute_pc(X,npc=1):\n",
    "    \"\"\"\n",
    "    Compute the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: component_[i,:] is the i-th pc\n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "def remove_pc(X, npc=1):\n",
    "    \"\"\"\n",
    "    Remove the projection on the principal components\n",
    "    :param X: X[i,:] is a data point\n",
    "    :param npc: number of principal components to remove\n",
    "    :return: XX[i, :] is the data point after removing its projection\n",
    "    \"\"\"\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc==1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "def get_norm_embedding(We, x1, w1, params):\n",
    "    '''basically does what the scoring function below does, but for one sentence'''\n",
    "    emb1 = get_weighted_average(We, x1, w1) # num samples by embedding dim\n",
    "    if  params.rmpc > 0:\n",
    "        emb1 = remove_pc(emb1, params.rmpc)\n",
    "    emb1norm = np.sqrt((emb1 * emb1).sum(axis=1))\n",
    "    return emb1, emb1norm\n",
    "\n",
    "def weighted_average_sim_rmpc(We,x1,x2,w1,w2, params):\n",
    "    \"\"\"\n",
    "    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n",
    "    :param We: We[i,:] is the vector for word i\n",
    "    :param x1: x1[i, :] are the indices of the words in the first sentence in pair i\n",
    "    :param x2: x2[i, :] are the indices of the words in the second sentence in pair i\n",
    "    :param w1: w1[i, :] are the weights for the words in the first sentence in pair i\n",
    "    :param w2: w2[i, :] are the weights for the words in the first sentence in pair i\n",
    "    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n",
    "    :return: scores, scores[i] is the matching score of the pair i\n",
    "    \"\"\"\n",
    "    emb1 = get_weighted_average(We, x1, w1)\n",
    "    emb2 = get_weighted_average(We, x2, w2)\n",
    "    if  params.rmpc > 0:\n",
    "        emb1 = remove_pc(emb1, params.rmpc)\n",
    "        emb2 = remove_pc(emb2, params.rmpc)\n",
    "\n",
    "    inn = (emb1 * emb2).sum(axis=1) # inner products\n",
    "    emb1norm = np.sqrt((emb1 * emb1).sum(axis=1))\n",
    "    emb2norm = np.sqrt((emb2 * emb2).sum(axis=1))\n",
    "    scores = inn / emb1norm / emb2norm # cosine similarity\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###                 Helper Methods for Clueweb-related tasks                ###\n",
    "###############################################################################\n",
    "\n",
    "### CLUEWEB\n",
    "def get_statistics(lst):\n",
    "    stats = {}\n",
    "    stats['mean'] = np.mean(lst)\n",
    "    stats['median'] = np.median(lst)\n",
    "    # stats['quartiles'] = \n",
    "    stats['standard deviation'] = np.std(lst)\n",
    "    stats['mean absolute deviation'] = np.mean(np.absolute(lst - np.mean(lst)))\n",
    "    stats['interquartile range'] = iqr(lst)\n",
    "\n",
    "    # plt.clera()\n",
    "    # plt.hist(lst, bins=100)\n",
    "    # plt.show()\n",
    "\n",
    "    return stats\n",
    "\n",
    "### CLUEWEB\n",
    "def get_embeddings_clueweb(We, words, data, weight4ind, params, key):\n",
    "    seq, labels, sentences = [], [], []\n",
    "    key = str((idx2entity[key[0]], idx2entity[key[1]], idx2entity[key[2]]))\n",
    "    for i, text_instance in enumerate(data):\n",
    "        # print '\\t\\t' + str(text_instance) + ' ' + str(idx_2_sent_map[text_instance])\n",
    "        ### remember text instances are tuples (\"text goes here\",)\n",
    "        X = data_io.getSeq(idx_2_sent_map[text_instance], words)\n",
    "        labels.append(key)\n",
    "        seq.append(X)\n",
    "        sentences.append(text_instance)\n",
    "\n",
    "    x1, w1 = data_io.prepare_data(seq)\n",
    "    emb, embnorm = get_norm_embedding(We, x1, w1, params)\n",
    "    emb = np.asarray(emb, dtype=np.float64)\n",
    "    # perhaps divide each row by its L2 norm, emb /= embnorm\n",
    "    return emb, labels, sentences\n",
    "\n",
    "### CLUEWEB\n",
    "def evaluate_similarity_all_pairs(We, words, data, weight4ind, scoring_function, params):\n",
    "    '''data here is a dictionary of 5k of the most commonly mentioned freebase triples \n",
    "        mapping to a list of their textual instances, which is at most 40k in length. \n",
    "        The goal here is to get a sense of how similar the instances in the list are with \n",
    "        each other - there are no labels here, just output say the average/median \n",
    "        similarity score of all pairs in the list, or a subsample of all pairs. \n",
    "    '''\n",
    "    triple_sim_stats = {}\n",
    "    avgs = []\n",
    "    prob = 0.001 \n",
    "    counter = 0\n",
    "    seq1, seq2 = [], []\n",
    "    keys = sorted(data, key=lambda x: len(data[x]), reverse=True)\n",
    "    try:\n",
    "        output = open('output_similarity_all_pairs.txt', 'w')\n",
    "    except:\n",
    "        print 'failure'\n",
    "        exit()\n",
    "        \n",
    "    for key in keys:\n",
    "        texts = data[key]\n",
    "        pairs = itertools.combinations(texts, 2)\n",
    "        counter = 10000\n",
    "        seq1, seq2 = [], []\n",
    "        output.write('-------------------\\n')\n",
    "        output.write(str(key) + '\\n')\n",
    "        output.write('number of textual instances: ' + str(len(texts)) + '\\n')\n",
    "\n",
    "        for i, pair in enumerate(pairs):\n",
    "            if random.random() > prob: ### subsample\n",
    "                continue\n",
    "            counter -= 1\n",
    "            if counter <= 0:\n",
    "                break\n",
    "            pair = (pair[0][0].strip(), pair[1][0].strip())\n",
    "            X1, X2 = data_io.getSeqs(pair[0],pair[1], words)\n",
    "            seq1.append(X1)\n",
    "            seq2.append(X2)\n",
    "        output.write('length of sampled sequence: ' + str(len(seq1)) + '\\n')\n",
    "        assert len(seq1) == len(seq2)\n",
    "\n",
    "        if len(seq1) == 0:\n",
    "            output.write('skipped this triple!\\n')\n",
    "            continue\n",
    "        x1,m1 = data_io.prepare_data(seq1)\n",
    "        x2,m2 = data_io.prepare_data(seq2)\n",
    "        m1 = data_io.seq2weight(x1, m1, weight4ind)\n",
    "        m2 = data_io.seq2weight(x2, m2, weight4ind)\n",
    "        scores = scoring_function(We,x1,x2,m1,m2, params)\n",
    "        preds = np.squeeze(scores)\n",
    "        stats = get_statistics(preds)\n",
    "        output.write(pprint.pformat(stats) + '\\n')\n",
    "        triple_sim_stats[key] = stats\n",
    "        avgs.append(stats['mean'])\n",
    "        print 'done with ' + str(key[0][0]) + '-' + key[1][0] + '-' + str(key[2][0])\n",
    "    output.close()\n",
    "    return triple_sim_stats, avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# mpld3.enable_notebook()\n",
    "mpld3.disable_notebook()\n",
    "\n",
    "###############################################################################\n",
    "###                 Manage and Visualize Clueweb Experiments                ###\n",
    "###############################################################################\n",
    "\n",
    "def t_sne(data):\n",
    "    '''run_bh_tsne(data, no_dims=2, perplexity=50, theta=0.5, randseed=-1, verbose=False,initial_dims=50, use_pca=True, max_iter=1000):\n",
    "        \n",
    "        Run TSNE based on the Barnes-HT algorithm\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        data: file or numpy.array\n",
    "            The data used to run TSNE, one sample per row\n",
    "        no_dims: int\n",
    "        perplexity: int\n",
    "        randseed: int\n",
    "        theta: float\n",
    "        initial_dims: int\n",
    "        verbose: boolean\n",
    "        use_pca: boolean\n",
    "        max_iter: int\n",
    "    '''\n",
    "\n",
    "    X_2d = bh_sne(data, perplexity=30, theta=0.5) #, max_iter=600) #bh_sne(X, perplexity=50, theta=0.5)  \n",
    "    return X_2d\n",
    "\n",
    "def tooltip_style(text):\n",
    "    return \"<p style=\\\"color: #ffffff; background-color: #000000\\\">\" + str(text) + \"</p>\"\n",
    "\n",
    "def plot_clueweb(X, Y, popup_labels, title):\n",
    "    '''X is a list of matrices of length n, each of which has its own class or \n",
    "    color, y. popup_labels is also a list of n lists, each containing the tag\n",
    "    of individual points (rows in a matrix of X)'''\n",
    "\n",
    "    ###############\n",
    "    labels_flat = [item for sublist in Y for item in sublist]\n",
    "    labels_set = set(labels_flat)\n",
    "            \n",
    "    fig, ax = plt.subplots(subplot_kw=dict(axisbg='#EEEEEE'), figsize=(20,20))\n",
    "    \n",
    "    assert len(X) == len(Y)\n",
    "    if popup_labels:\n",
    "        assert len(popup_labels) == len(X)\n",
    "        assert np.shape(X)[0] == np.shape(popup_labels)[0]\n",
    "    ax.grid(color='white', linestyle='solid')\n",
    "    colors = cm.rainbow(np.linspace(0,1, len(Y)))\n",
    "    color_label_map = {}\n",
    "    for c, label in zip(colors, labels_set):\n",
    "        color_label_map[label] = c\n",
    "#     print 'labels\\' colors: ' + str(color_label_map)\n",
    "        \n",
    "    legend_partition, legend_strings = [], [] \n",
    "    for (x, group_labels, tag) in zip(X, Y, popup_labels):\n",
    "        label = group_labels[0]\n",
    "        assert all(label == i for i in group_labels)\n",
    "        scatter = ax.scatter(x[:, 0], x[:, 1], c = color_label_map[label], alpha=0.3, label=label)\n",
    "        legend_partition.append(scatter)\n",
    "        legend_strings.append(label)\n",
    "#         tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=[tooltip_style(t[0]) for t in tag])\n",
    "        tooltip = mpld3.plugins.PointHTMLTooltip(scatter, labels=[tooltip_style(t[0]) for t in tag])\n",
    "        mpld3.plugins.connect(fig, tooltip)\n",
    "        \n",
    "    l_keys, l_nums, recs = [], [], []\n",
    "    print 'Legend: '\n",
    "    for i, (key, value) in enumerate(color_label_map.items()):\n",
    "        print str(i) + ' = ' + str(key)\n",
    "        l_nums.append(str(i))\n",
    "        l_keys.append(key)\n",
    "        recs.append(mpatches.Rectangle((0,0),1,1,fc=value))\n",
    "\n",
    "    plt.legend(recs, l_keys, loc='upper right', fontsize=8)\n",
    "    if title:\n",
    "        ax.set_title(title, size=12)\n",
    "        \n",
    "    mpld3.save_html(fig, title + '.html')\n",
    "    mpld3.show()\n",
    "#     mpld3.display() # doesn't seem to work for ipython notebooks?\n",
    "    return\n",
    "\n",
    "def clueweb_tsne_experiment():\n",
    "     ### T-SNE on selected triples - A COUPLE OF EXPERIMENTS NEED TO BE DONE HERE!!!!\n",
    "    with open(data_path + paraphrase_file) as g:\n",
    "        ### only load textual mentions of triples with the most mentions\n",
    "        triples = []\n",
    "        for i in g.readlines():\n",
    "            i = i.strip().split()\n",
    "            triple = (entity2idx[i[0]], entity2idx[i[1]], entity2idx[i[2]])\n",
    "            triples.append(triple)\n",
    "            \n",
    "        X, labels, sentences = [], [], []\n",
    "        for t in triples:\n",
    "            key = str((idx2entity[t[0]], idx2entity[t[1]], idx2entity[t[2]]))\n",
    "            data_selected = [i for i in unique_text_per_triple[t] if triple_per_text[i] <= 6] ### a set of text instances\n",
    "            print '\\t' + key + ' num valid texts: ' + str(len(data_selected)) \\\n",
    "                + ' out of num unique texts: ' + str(len(unique_text_per_triple[t]))\n",
    "            if len(data_selected) <= 0:\n",
    "                continue\n",
    "            x, labels_x, sentences_x = get_embeddings_clueweb(We, words, data_selected, weight4ind, params, t)\n",
    "            print '\\tshape: ' + str(np.shape(x))\n",
    "            if np.shape(x)[0] > 300:  ### sample 200 instances to reduce runtime\n",
    "                idx = np.random.choice(np.shape(x)[0], 300)\n",
    "                X.append(x[idx, :])\n",
    "                labels.append((np.array(labels_x)[idx]).tolist())\n",
    "                sentences.append((np.array(sentences_x)[idx]).tolist())\n",
    "            else:\n",
    "                X.append(x)\n",
    "                labels.append(labels_x)\n",
    "                sentences.append(sentences_x)\n",
    "            print '\\t' + str(np.shape(X[-1])) + ' ' + str((idx2entity[t[0]], idx2entity[t[1]], idx2entity[t[2]]))\n",
    "        print 'done creating embeddings'\n",
    "        X_stacked = np.asarray(np.vstack(tuple(X)), dtype=np.float64)\n",
    "        X_2d = t_sne(X_stacked) #\n",
    "        print 'done with tsne ' + str(np.shape(X_2d)) \n",
    "\n",
    "        ### must chunk the t-sne embeddings to match labels...\n",
    "        X_2d_chunk = []\n",
    "        size = 0\n",
    "        for i in labels:\n",
    "            X_2d_chunk.append(X_2d[size:(size + len(i))])\n",
    "            size += len(i)\n",
    "\n",
    "        for a, b, c in zip(X_2d_chunk, labels, sentences):\n",
    "            assert len(a) == len(b) == len(c)\n",
    "        return X_2d_chunk, labels, sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "\n",
      "done with word2weight\n",
      "done with weight4ind\n",
      "word vectors loaded from /Users/corbinrosset/Dropbox/Paragrams/paragrams-XXL-SL999.txt\n",
      "word weights computed from /Users/corbinrosset/Dropbox/Arora/QA-code/src/WordEmdgs/SIF/auxiliary_data/enwiki_vocab_min200.txt using parameter a=0.010000\n",
      "*** remove the first 0 principal components ***\n",
      "\t('/m/0d19y2', '/medicine/disease/risk_factors', '/m/07jwr') num valid texts: 276 out of num unique texts: 323\n",
      "\tshape: (276, 300)\n",
      "\t(200, 300) ('/m/0d19y2', '/medicine/disease/risk_factors', '/m/07jwr')\n",
      "\t('/m/05qtj', '/architecture/museum/address./location/mailing_address/citytown', '/m/04gdr') num valid texts: 405 out of num unique texts: 624\n",
      "\tshape: (405, 300)\n",
      "\t(200, 300) ('/m/05qtj', '/architecture/museum/address./location/mailing_address/citytown', '/m/04gdr')\n",
      "\t('/m/02h40lc', '/education/field_of_study/students_majoring./education/education/major_field_of_study', '/m/02bjrlw') num valid texts: 55 out of num unique texts: 106\n",
      "\tshape: (55, 300)\n",
      "\t(55, 300) ('/m/02h40lc', '/education/field_of_study/students_majoring./education/education/major_field_of_study', '/m/02bjrlw')\n",
      "\t('/m/03_c8p', '/business/consumer_product/company./business/company_product_relationship/company', '/m/067gh') num valid texts: 344 out of num unique texts: 500\n",
      "\tshape: (344, 300)\n",
      "\t(200, 300) ('/m/03_c8p', '/business/consumer_product/company./business/company_product_relationship/company', '/m/067gh')\n",
      "\t('/m/09c7w0', '/olympics/olympic_participating_country/medals_won./olympics/olympic_medal_honor/olympics', '/m/0kbws') num valid texts: 580 out of num unique texts: 904\n",
      "\tshape: (580, 300)\n",
      "\t(200, 300) ('/m/09c7w0', '/olympics/olympic_participating_country/medals_won./olympics/olympic_medal_honor/olympics', '/m/0kbws')\n",
      "\t('/m/04r_8', '/law/inventor/inventions', '/m/04sv4') num valid texts: 1499 out of num unique texts: 2274\n",
      "\tshape: (1499, 300)\n",
      "\t(200, 300) ('/m/04r_8', '/law/inventor/inventions', '/m/04sv4')\n",
      "\t('/m/045m1_', '/religion/religion/deities', '/m/01lp8') num valid texts: 1191 out of num unique texts: 1653\n",
      "\tshape: (1191, 300)\n",
      "\t(200, 300) ('/m/045m1_', '/religion/religion/deities', '/m/01lp8')\n",
      "done creating embeddings\n",
      "done with tsne (1255, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f6de043d6a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mX_2d_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclueweb_tsne_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mplot_clueweb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_2d_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m't-SNE of textual instances of most common clueweb triples (6) (sampled)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'done'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6fc26723fc9a>\u001b[0m in \u001b[0;36mplot_clueweb\u001b[0;34m(X, Y, popup_labels, title)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mlegend_strings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#         tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=[tooltip_style(t[0]) for t in tag])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtooltip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpld3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPointHTMLTooltip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtooltip_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mmpld3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtooltip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "###                                 Main                                    ###\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "### tune weight and rmpc params in evaluation of similarity\n",
    "for weightpara in weightparas:\n",
    "    print '\\n==================================\\n'\n",
    "    #### load weight for each word using idf or tf-idf\n",
    "    word2weight = data_io.getWordWeight(weightfile, weightpara)\n",
    "    print 'done with word2weight' \n",
    "    weight4ind = data_io.getWeight(words, word2weight)\n",
    "    print 'done with weight4ind'\n",
    "    \n",
    "    for rmpc in rmpcs:\n",
    "        print 'word vectors loaded from %s' % wordfile\n",
    "        print 'word weights computed from %s using parameter a=%f' % (weightfile, weightpara)\n",
    "        params.rmpc = rmpc\n",
    "        print '*** remove the first %d principal components ***' % rmpc\n",
    "\n",
    "        #################################################################\n",
    "        ### modify the following lines for your purposes (clueweb or sim)\n",
    "        #################################################################\n",
    "\n",
    "\n",
    "        #################################################################\n",
    "        #################################################################\n",
    "        ### for text similarity task data:\n",
    "\n",
    "        # for file in paraphrases_files:\n",
    "        # scoring_function = weighted_average_sim_rmpc\n",
    "        # triple_sim_stats, avgs = evaluate_similarity(We, words, data, weight4ind, scoring_function, params) ### only for clueweb\n",
    "\n",
    "        # print 'average similarities per triple: ' + str(avgs)\n",
    "        # print 'average of averages: ' + str(np.mean(avgs))\n",
    "        # with open(data_dir + 'clueweb_triples_similarity_' + str(rmpc) + '_rmpc_' + str(weightpara) + '_weight.pkl', 'w') as f:\n",
    "        #     pickle.dump(triple_sim_stats, f)\n",
    "\n",
    "        #################################################################\n",
    "        ### T-SNE\n",
    "        # X, labels, sentences = get_embeddings(We, words, data, weight4ind, params)\n",
    "        # t_sne(X, labels, sentences, 't-SNE on text similarity pairs')\n",
    "\n",
    "        #################################################################\n",
    "        #################################################################\n",
    "        ### for clueweb data\n",
    "\n",
    "        ### test similirity of textual instances of same FB triple...\n",
    "#         scoring_function = weighted_average_sim_rmpc\n",
    "#         triple_sim_stats, avgs = evaluate_similarity_all_pairs(We, words, DATA, weight4ind, scoring_function, params) ### only for clueweb\n",
    "#         print 'average similarities per triple: ' + str(avgs)\n",
    "#         print 'average of averages: ' + str(np.mean(avgs))\n",
    "#         with open(data_dir + 'clueweb_triples_similarity_' + str(rmpc) + '_rmpc_' + str(weightpara) + '_weight.pkl', 'w') as f:\n",
    "#             pickle.dump(triple_sim_stats, f)\n",
    "\n",
    "        #################################################################\n",
    "        X_2d_chunk, labels, sentences = clueweb_tsne_experiment()\n",
    "        plot_clueweb(X_2d_chunk, labels, sentences, title='t-SNE of textual instances of most common clueweb triples (6) (sampled)')\n",
    "        print 'done'\n",
    "       \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "###                                 Dead                                    ###\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
